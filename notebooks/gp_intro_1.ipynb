{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-parametric Bayes: Gaussian Processes\n",
    "\n",
    "Use of the term \"non-parametric\" in the context of Bayesian analysis is something of a misnomer. This is because the first and fundamental step in Bayesian modeling is to specify a *full probability model* for the problem at hand. It is rather difficult to explicitly state a full probability model without the use of probability functions, which are parametric. Bayesian non-parametric methods do not imply that there are no parameters, but rather that the number of parameters grows with the size of the dataset. In fact, Bayesian non-parametric models are *infinitely* parametric.\n",
    "\n",
    "## Building models with Gaussians\n",
    "\n",
    "It is easy to develop large, parametric models $p(y|\\theta)$ for large $\\theta$, particularly in a Bayesian context. However, this usually results in having to work with multidimensional integration over $\\theta$. \n",
    "\n",
    "One approach is to use MCMC; another is to represent your model with Gaussians. Normal distributions are easier to work with.\n",
    "\n",
    "$$p(x \\mid \\pi, \\Sigma) = (2\\pi)^{-k/2}|\\Sigma|^{-1/2} \\exp\\left\\{ -\\frac{1}{2} (x-\\mu)^{\\prime}\\Sigma^{-1}(x-\\mu) \\right\\}$$\n",
    "\n",
    "* marginals of multivariate normal distributions are normal\n",
    "\n",
    "$$p(x,y) = \\mathcal{N}\\left(\\left[{\n",
    "\\begin{array}{c}\n",
    "  {\\mu_x}  \\\\\n",
    "  {\\mu_y}  \\\\\n",
    "\\end{array}\n",
    "}\\right], \\left[{\n",
    "\\begin{array}{c}\n",
    "  {\\Sigma_x} & {\\Sigma_{xy}}  \\\\\n",
    "  {\\Sigma_{xy}^T} & {\\Sigma_y}  \\\\\n",
    "\\end{array}\n",
    "}\\right]\\right)$$\n",
    "\n",
    "$$p(x) = \\int p(x,y) dy = \\mathcal{N}(\\mu_x, \\Sigma_x)$$\n",
    "\n",
    "* conditionals of multivariate normals are normal\n",
    "\n",
    "$$p(x|y) = \\mathcal{N}(\\mu_x + \\Sigma_{xy}\\Sigma_y^{-1}(y-\\mu_y), \n",
    "\\Sigma_x-\\Sigma_{xy}\\Sigma_y^{-1}\\Sigma_{xy}^T)$$\n",
    "\n",
    "\n",
    "In some situations, we want to gain infernce about *functions*, rather than about, say, individuals or small vectors of parameters.\n",
    "\n",
    "A Gaussian process generalizes the multivariate normal to infinite dimension. It is considered a non-parametric approach, despite having an infinite number of parameters.\n",
    "\n",
    "**Gaussian Process**\n",
    "\n",
    "> An infinite collection of random variables, any finite subset of which have a Gaussian distribution.\n",
    "\n",
    "A Gaussian process is a ***disribution over functions***. Just as a multivariate normal distribution is completely specified by a mean vector and covariance matrix, a GP is fully specified by a mean *function* and a covariance *function*:\n",
    "\n",
    "$$p(x) \\sim \\mathcal{GP}(m(x), k(x,x^{\\prime}))$$\n",
    "\n",
    "It is the marginalization property that makes working with a Gaussian process feasible: we can marginalize over the infinitely-many variables that we are not interested in, or have not observed. \n",
    "\n",
    "For example, one specification of a GP might be as follows:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "m(x) &=0 \\\\\n",
    "k(x,x^{\\prime}) &= \\theta_1\\exp\\left(-\\frac{\\theta_2}{2}(x-x^{\\prime})^2\\right)\n",
    "\\end{aligned}$$\n",
    "\n",
    "here, the covariance function is a squared exponential, for which values of $x$ and $x^{\\prime}$ that are close together result in values of $k$ closer to 1 and those that are far apart return values closer to zero. (*spoiler*: we usually aren't very interested in the mean function!).\n",
    "\n",
    "For a finite number of points, the GP becomes a multivariate normal, with the mean and covariance as the mean functon and covariance function evaluated at those points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Basic exponential kernel\n",
    "exponential_kernel = lambda x, y, params: params[0] * \\\n",
    "    np.exp( -0.5 * params[1] * np.sum((x - y)**2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate a random sample from a GP with mean function zero and a double exponential covariance function as follows. \n",
    "\n",
    "In the context of Gaussian Processes, the covariance matrix is  referred to as the kernel (or Gram) matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Covariance matrix for MV normal\n",
    "covariance = lambda kernel, x, y, params: \\\n",
    "    np.array([[kernel(xi, yi, params) for xi in x] for yi in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.random.randn(10)*2\n",
    "theta = [1, 10]\n",
    "sigma = covariance(exponential_kernel, x, x, theta)\n",
    "y = np.random.multivariate_normal(np.zeros(len(x)), sigma)\n",
    "plt.plot(x, y, 'bo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate sample functions (*realizations*) sequentially, using the conditional:\n",
    "\n",
    "$$p(x|y) = \\mathcal{N}(\\mu_x + \\Sigma_{xy}\\Sigma_y^{-1}(y-\\mu_y), \n",
    "\\Sigma_x-\\Sigma_{xy}\\Sigma_y^{-1}\\Sigma_{xy}^T)$$\n",
    "\n",
    "This function implements the conditional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def conditional(x_new, x, y, fcov=exponential_kernel, params=theta):\n",
    "    B = covariance(fcov, x_new, x, params)\n",
    "    C = covariance(fcov, x, x, params)\n",
    "    A = covariance(fcov, x_new, x_new, params)\n",
    "    mu = np.linalg.inv(C).dot(B).T.dot(y)\n",
    "    sigma = A - np.linalg.inv(C).dot(B).T.dot(B)\n",
    "    return mu.squeeze(), sigma.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This band represents the prior mean function, plus and minus one standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sigma0 = exponential_kernel(0, 0, theta)\n",
    "xpts = np.arange(-3, 3, step=0.01)\n",
    "plt.errorbar(xpts, np.zeros(len(xpts)), yerr=sigma0, capsize=0)\n",
    "plt.ylim(-3, 3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by selecting a point at random, then drawing from an *unconditional* Gaussian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = [1.]\n",
    "y = [np.random.normal(scale=sigma0)]\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now calculate the conditional distribution, given the point that we just sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sigma1 = covariance(exponential_kernel, x, x, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(x, data, kernel, params, sigma, t):\n",
    "    k = [kernel(x, y, params) for y in data]\n",
    "    Sinv = np.linalg.inv(sigma)\n",
    "    y_pred = np.dot(k, Sinv).dot(t)\n",
    "    sigma_new = kernel(x, x, params) - np.dot(k, Sinv).dot(k)\n",
    "    return y_pred, sigma_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_pred = np.linspace(-3, 3, 1000)\n",
    "predictions = [predict(i, x, exponential_kernel, theta, sigma1, y) \n",
    "               for i in x_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot this to get an idea of what this looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred, sigmas = np.transpose(predictions)\n",
    "plt.errorbar(x_pred, y_pred, yerr=sigmas, capsize=0)\n",
    "plt.plot(x, y, \"ro\")\n",
    "plt.xlim(-3, 3); plt.ylim(-3, 3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can select a second point, conditional on the first point, using this new distribution. Let's arbitrarily select one at $x=-0.7$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu2, s2 = conditional([-0.7], x, y)\n",
    "y2 = np.random.normal(mu2, s2)\n",
    "y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.append(-0.7)\n",
    "y.append(y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the conditional distribution again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sigma2 = covariance(exponential_kernel, x, x, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = [predict(i, x, exponential_kernel, theta, sigma2, y) \n",
    "               for i in x_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred, sigmas = np.transpose(predictions)\n",
    "plt.errorbar(x_pred, y_pred, yerr=sigmas, capsize=0)\n",
    "plt.plot(x, y, \"ro\")\n",
    "plt.xlim(-3, 3); plt.ylim(-3, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the existing points constrain the selection of subsequent points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_more = [-2.1, -1.5, 0.3, 1.8, 2.5]\n",
    "mu, s = conditional(x_more, x, y)\n",
    "y_more = np.random.multivariate_normal(mu, s)\n",
    "y_more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x += x_more\n",
    "y += y_more.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sigma_new = covariance(exponential_kernel, x, x, theta)\n",
    "\n",
    "predictions = [predict(i, x, exponential_kernel, theta, sigma_new, y) \n",
    "               for i in x_pred]\n",
    "\n",
    "y_pred, sigmas = np.transpose(predictions)\n",
    "plt.errorbar(x_pred, y_pred, yerr=sigmas, capsize=0)\n",
    "plt.plot(x, y, \"ro\")\n",
    "plt.ylim(-3, 3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is exactly equivalent to adding points simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not restricted to a double-exponential covariance function. Here is a slightly more general function, which includes a constant and linear term, in addition to the exponential.\n",
    "\n",
    "$$k(x,x\\prime) = \\theta_1 \\exp\\left(-\\frac{\\theta_2}{2}(x-x^{\\prime})^2\\right) + \\theta_3 + \\theta_4 x^{\\prime} x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Exponential kernel, plus constant and linear terms\n",
    "exponential_linear_kernel = lambda x, y, params: \\\n",
    "    exponential_kernel(x, y, params[:2]) + params[2] + params[3] * np.dot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameters for the expanded exponential kernel\n",
    "theta = 2.0, 50.0, 0.0, 1.0\n",
    "\n",
    "# Some sample training points.\n",
    "xvals = np.random.rand(10) * 2 - 1\n",
    "\n",
    "# Construct the Gram matrix\n",
    "C = covariance(exponential_linear_kernel, xvals, xvals, theta)\n",
    "\n",
    "# Sample from the multivariate normal\n",
    "yvals = np.random.multivariate_normal(np.zeros(len(xvals)), C)\n",
    "\n",
    "plt.plot(xvals, yvals, \"ro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_pred = np.linspace(-1, 1, 1000)\n",
    "predictions = [predict(i, xvals, exponential_linear_kernel, theta, C, yvals) \n",
    "               for i in x_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y, sigma = np.transpose(predictions)\n",
    "plt.errorbar(x_pred, y, yerr=sigma, capsize=0)\n",
    "plt.plot(xvals, yvals, \"ro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the GP as a prior, and update it using data (rather than random realizations), to obtain a posterior GP that we can use for prediction, conditional on the data.\n",
    "\n",
    "## Marginal Likelihood\n",
    "\n",
    "The marginal likelihood is the normalizing constant for the posterior distribution, and is the integral of the product of the likelihood and prior.\n",
    "\n",
    "$$p(y|X) = \\int_f p(y|f,X)p(f|X) df$$\n",
    "\n",
    "where for Gaussian processes, we are marginalizing over function values $f$ (instead of parameters $\\theta$).\n",
    "\n",
    "GP prior:\n",
    "\n",
    "$$\\log p(f|X) = - \\frac{k}{2}\\log2\\pi - \\frac{1}{2}\\log|K| -\\frac{1}{2}f^TK^{-1}f $$\n",
    "\n",
    "Gaussian likelihood:\n",
    "\n",
    "$$\\log p(y|f,X) = - \\frac{k}{2}\\log2\\pi - \\frac{1}{2}\\log|\\sigma^2I| -\\frac{1}{2}(y-f)^T(\\sigma^2I)^{-1}(y-f) $$\n",
    "\n",
    "Marginal likelihood:\n",
    "\n",
    "$$\\log p(y|X) = - \\frac{k}{2}\\log2\\pi - \\frac{1}{2}\\log|K + \\sigma^2I| - \\frac{1}{2}y^T(K+\\sigma^2I)^{-1}y $$\n",
    "\n",
    "Notice that the marginal likelihood includes both a data fit term $- \\frac{1}{2}y^T(K+\\sigma^2I)^{-1}y$ and a parameter penalty term $\\frac{1}{2}\\log|K + \\sigma^2I|$. Hence, the marginal likelihood can help us select an appropriate covariance function, based on its fit to the dataset at hand.\n",
    "\n",
    "### Choosing parameters\n",
    "\n",
    "This is relevant because we have to make choices regarding the parameters of our Gaussian process; they were chosen arbitrarily for the random functions we demonstrated above.\n",
    "\n",
    "For example, in the squared exponential covariance function, we must choose two parameters:\n",
    "\n",
    "$$k(x,x^{\\prime}) = \\theta_1\\exp\\left(-\\frac{\\theta_2}{2}(x-x^{\\prime})^2\\right)$$\n",
    "\n",
    "The first parameter $\\theta_1$ is a scale parameter, which allows the function to yield values outside of the unit interval. The second parameter $\\theta_2$ is a length scale parameter that determines the degree of covariance between $x$ and $x^{\\prime}$; smaller values will tend to smooth the function relative to larger values.\n",
    "\n",
    "We can use the **marginal likelihood** to select appropriate values for these parameters, since it trades off model fit with model complexity. Thus, an optimization procedure can be used to select values for $\\theta$ that maximize the marginial likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These examples, of course, are trivial because they are simply random functions. What we are really interested in is *learning* about an underlying function from information residing in our data. In a parametric setting, we either specify a likelihood, which we then maximize with respect to the parameters, of a full probability model, for which we calculate the posterior in a Bayesian context. Though the integrals associated with posterior distributions are typically intractable for parametric models, they do not pose a problem with Gaussian processes.\n",
    "\n",
    "## Gaussian process priors\n",
    "\n",
    "We can treat our zero-mean (or otherwise arbitrary) Gaussian process as a prior for our model. If we are able to use a Gaussian as our data likelihood, then we can construct a Gaussian proceess posterior.\n",
    "\n",
    "Keeping in mind that a Gaussian process is a distribution over functions, rather than parameters, our likelihood takes the following form:\n",
    "\n",
    "$$y|p(x),x \\sim \\mathcal{N}(p(x), \\sigma^2I) $$\n",
    "\n",
    "Here, $\\sigma^2$ represents observation error, or noise, and our unknown is a function.\n",
    "\n",
    "Notice that the GP likelihood conditions on a function; however, we are only interested in the function at locations where we have data!\n",
    "\n",
    "The corresponding prior is:\n",
    "\n",
    "$$p(x) \\sim \\mathcal{GP}(m_0(x), k_0(x,x\\prime))$$\n",
    "\n",
    "Multiplying an infinite normal with another infinite normal yields another infinite normal, our posterior process:\n",
    "\n",
    "$$p(x)|y \\sim \\mathcal{GP}(m_{post}, k_{post}(x,x^{\\prime}))$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\begin{aligned}\n",
    "m_{post} &= k(x,x^{\\prime})^T[k(x,x) + \\sigma^2I]^{-1}y \\\\\n",
    "k_{post}(x,x^{\\prime}) &= k(x^{\\prime},x^{\\prime}) - k(x,x^{\\prime})^T[k(x,x) + \\sigma^2I]^{-1}k(x,x^{\\prime})\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior predictive distribution for the GP is specified by:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "m^* &= k(x^*,x)^T[k(x,x) + \\sigma^2I]^{-1}y \\\\\n",
    "k^*(x^*,x) &= k(x^*,x^*)+\\sigma^2 - k(x^*,x)^T[k(x,x) + \\sigma^2I]^{-1}k(x^*,x)\n",
    "\\end{aligned}$$\n",
    "\n",
    "Though this calculation may seem straightforward, but note that the mean and covariate calculations involve inversions $k(x,x)$, which is a $\\mathcal{O}(n^3)$ computation. Thus, Gaussian processes as presented are usually only feasible for data up to a few thousand observations in size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Processes in PyMC\n",
    "\n",
    "Along with standard parametric Bayesian models, PyMC 2 includes a module `gp` for fitting non-parametric models using Gaussian processes.\n",
    "\n",
    "### Mean function\n",
    "\n",
    "The mean function of a GP can be interpreted as a \"prior guess\" at the form of the true function. Typically, we use a zero mean function (or some linear function), as we have seen above, but we can choose from a range of alternatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymc import gp\n",
    "\n",
    "# Generate mean\n",
    "def quadfun(x, a, b, c):\n",
    "    return (a*x**2 + b*x + c)\n",
    "\n",
    "M = gp.Mean(quadfun, a=1., b=0.5, c=2.)\n",
    "\n",
    "x = np.arange(-1,1,0.1)\n",
    "plt.plot(x, M(x), 'k-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance function\n",
    "\n",
    "The behavior of individual realizations from the GP is governed by the covariance function. This function controls both the degree of *shrinkage* to the mean function and the *smoothness* of functions sampled from the GP.\n",
    "\n",
    "The Mat&#232;rn class of functions is a flexible choice.\n",
    "\n",
    "$$k_{Matern}(d) = \\frac{\\sigma^2}{\\Gamma(\\nu)2^{\\nu-1}} \\left(\\frac{\\sqrt{2 \\nu} d}{l}\\right)^{\\nu} K_{\\nu}\\left(\\frac{\\sqrt{2 \\nu} d}{l}\\right)$$\n",
    "\n",
    "The Mat&#232;rn covariance function has three parameters, each of which clearly controls one of three important properties of realizations.\n",
    "\n",
    "**amplitude** ($\\sigma$)\n",
    "\n",
    "The amplitude parameter (`amp` in PyMC) is a multiplier for realizations from the function that essentially stretches or compresses the y-axis.\n",
    "\n",
    "**lengthscale of changes** ($l$)\n",
    "\n",
    "The lengthscale parameter (`scale`) similarly scales realizations on the x-axis. Larger (greater than 1) values make points appear closer together.\n",
    "\n",
    "**roughness** ($\\nu$)\n",
    "\n",
    "The roughness parameter (`diff_degree`) controls the sharpness of the ridge of the covariance function, which in turn affects the roughness (or smoothness) of realizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymc.gp.cov_funs import matern\n",
    "\n",
    "C = gp.Covariance(eval_fun=matern.euclidean, diff_degree=1.4, amp=0.4, \n",
    "                  scale=1, rank_limit=1000)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.contourf(x, x, C(x,x).view(np.ndarray), origin='lower', extent=(-1,1,-1,1), \n",
    "            cmap=plt.cm.bone)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(x, C(x,0).view(np.ndarray), 'k-')\n",
    "plt.ylabel('C(x,0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Returns the diagnonal\n",
    "C(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that the covariance functions described above use a single parameter for controlling each of the attributes of the GP realizations. This implies that in a multivariate setting, such a covariance function would force the covariates to be isotropic in their influence on the response. If this is unreasonable, then the covariance function must be generalized to produce an anisotropic GP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawing realizations from a GP\n",
    "\n",
    "Since a Gaussian process is a distribution over functions, sampling from it yields functions rather than points. We refer to draws from GPs as *realizations*, which are represented using `Realization` objects in PyMC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate realizations\n",
    "f_list = [gp.Realization(M, C) for i in range(3)]\n",
    "\n",
    "# Plot mean and covariance\n",
    "x = np.arange(-1,1,0.01)\n",
    "gp.plot_envelope(M, C, x)\n",
    "\n",
    "# Add realizations\n",
    "for f in f_list:\n",
    "    plt.plot(x, f(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a GP to model observations as coming from a function for which we are highly uncertain. We can model this data as:\n",
    "\n",
    "$$ \\text{data}_i \\sim \\text{N}(f(o_i), V_i) $$\n",
    "\n",
    "which assumes only that the observation error is normally distributed. To represent the uncertainty regarding the expected value, we use a Gaussian process prior: \n",
    "\n",
    "$$ f \\sim \\text{GP}(M_o, C_o) $$ \n",
    "\n",
    "Combining these yields a posterior for *f* that is also a Gaussian process, with new mean and covariance functions:\n",
    "\n",
    "$$ f|\\text{data} \\sim \\text{GP}(M, C) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M = gp.Mean(quadfun, a=1., b=0.5, c=2.)\n",
    "C = gp.Covariance(eval_fun=matern.euclidean, diff_degree=1.4, \n",
    "                  amp=0.4, scale=1, rank_limit=1000)\n",
    "\n",
    "obs_x = np.array([-.5, .5])\n",
    "V = np.array([0.002, 0.002])\n",
    "data = np.array([3.1, 2.9])\n",
    "\n",
    "gp.observe(M=M, C=C, obs_mesh=obs_x, obs_V=V, obs_vals=data)\n",
    "\n",
    "# Generate realizations from posterior\n",
    "f_list = [gp.Realization(M,C) for i in range(3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `observe` informs the mean and covariance functions that values on `obs_mesh` with observation variance `V`. Making observations with no error is called `conditioning`. This is useful when, for example, forcing a rate function to be zero when a population's size is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gp.plot_envelope(M, C, mesh=x)\n",
    "for f in f_list:\n",
    "    plt.plot(x, f(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Sockeye salmon spawning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sockeye_data = np.reshape([2986,9,\n",
    "3424,12.39,\n",
    "1631,4.5,\n",
    "784,2.56,\n",
    "9671,32.62,\n",
    "2519,8.19,\n",
    "1520,4.51,\n",
    "6418,15.21,\n",
    "10857,35.05,\n",
    "15044,36.85,\n",
    "10287,25.68,\n",
    "16525,52.75,\n",
    "19172,19.52,\n",
    "17527,40.98,\n",
    "11424,26.67,\n",
    "24043,52.6,\n",
    "10244,21.62,\n",
    "30983,56.05,\n",
    "12037,29.31,\n",
    "25098,45.4,\n",
    "11362,18.88,\n",
    "24375,19.14,\n",
    "18281,33.77,\n",
    "14192,20.44,\n",
    "7527,21.66,\n",
    "6061,18.22,\n",
    "15536,42.9,\n",
    "18080,46.09,\n",
    "17354,38.82,\n",
    "17301,42.22,\n",
    "11486,21.96,\n",
    "20120,45.05,\n",
    "10700,13.7,\n",
    "12867,27.71,], (34,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "abundance = sockeye_data[:,0].ravel()\n",
    "frye = sockeye_data[:,1].ravel()\n",
    "\n",
    "# Function for prior mean\n",
    "line = lambda x, slope: slope * x\n",
    "\n",
    "M = gp.Mean(line, slope=(frye / abundance).mean())\n",
    "\n",
    "C = gp.Covariance(matern.euclidean,\n",
    "            diff_degree=1.4,\n",
    "            scale=100.*abundance.max(),\n",
    "            amp=200.*frye.max())\n",
    "\n",
    "#gp.observe(M, C, obs_mesh=0, obs_vals=0, obs_V=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Observe some data\n",
    "gp.observe(M, C, obs_mesh=abundance, \n",
    "        obs_vals=frye, obs_V=10*frye)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xplot = np.linspace(0,1.25 * abundance.max(),100)\n",
    "\n",
    "gp.plot_envelope(M, C, xplot)\n",
    "\n",
    "for i in range(50):\n",
    "    f = gp.Realization(M, C)\n",
    "    plt.plot(xplot,f(xplot),alpha=0.3)\n",
    "\n",
    "plt.plot(abundance, frye, 'k.', markersize=4)\n",
    "plt.xlabel('Female abundance')\n",
    "plt.ylabel('Frye density')\n",
    "plt.title('Sockeye salmon recritment')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Process Prior and MCMC\n",
    "\n",
    "The flexibility of Gaussian processes compels us to consider them as functional forms for prior distributions, and to fit them using MCMC. However, since a `GaussianProcess` object is a stochastic whose value is a `Realization` object, we cannot simply endow a it with a `logp` attribute, and so `GaussianProcess` objects cannot be handled by PyMC’s standard MCMC machinery. However, we can evaluate the GP on a `obs_mesh`, which yields a simple multivariate normal random variable that can be handled by PyMC. Thus, the `GPSubmodel` class is a container for `GaussianProcess` objects that performs this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymc import Gamma, deterministic, observed, MCMC\n",
    "from pymc import InverseGamma, Uniform, normal_like\n",
    "\n",
    "class SalmonSampler(MCMC):\n",
    "\n",
    "    def __init__(self, abundance, frye):\n",
    "\n",
    "        lfrye = np.log(frye)\n",
    "        labundance = np.log(abundance)\n",
    "\n",
    "        rx = labundance.max() - labundance.min()\n",
    "        ry = lfrye.max() - lfrye.min()\n",
    "\n",
    "\n",
    "        self.abundance = abundance\n",
    "        self.frye = frye\n",
    "        self.lfrye = np.log(frye)\n",
    "        self.labundance = np.log(abundance)\n",
    "        self.plot_x = np.linspace(self.abundance.min()*.1,self.abundance.max(),100)\n",
    "\n",
    "        # The mean function's parameters\n",
    "        beta_0 = Gamma('beta_0', alpha = np.log(4.5), beta = 1./(10.*(ry/4.)**2))\n",
    "        beta_1 = Gamma('beta_1', alpha = 1.6 * np.log(1000.) / ry**2, beta = 1.6 * np.log(1000.) / ry**2)\n",
    "\n",
    "        # The covariance function's parameters\n",
    "        invtausq = Gamma('invtausq', alpha = 2., beta = 1./(10.*(ry/4.)**2))\n",
    "\n",
    "        @deterministic\n",
    "        def amp(invtausq=invtausq):\n",
    "            \"\"\"\n",
    "            Prior amplitude of f.\n",
    "            \"\"\"\n",
    "            return 1./np.sqrt(invtausq)\n",
    "\n",
    "        scale = InverseGamma('scale' , alpha=2., beta=1./(6. / rx), value=3)\n",
    "        diff_degree = Uniform('diff_degree', .1, 3, value=1.5)\n",
    "\n",
    "        @deterministic\n",
    "        def C(diff_degree=diff_degree, amp=amp, scale=scale):\n",
    "            \"\"\"\n",
    "            The Matern covariance function, observed to be zero at the origin.\n",
    "            \"\"\"\n",
    "            C = gp.Covariance(matern.euclidean, diff_degree=diff_degree, amp=amp, scale=scale)\n",
    "            return C\n",
    "\n",
    "        @deterministic\n",
    "        def M(beta_0 = beta_0, beta_1 = beta_1):\n",
    "            \"\"\"\n",
    "            The mean function is the Cushing stock-recruitment function\n",
    "            \"\"\"\n",
    "            M = gp.Mean(lambda x: beta_0+ x*beta_1)\n",
    "            return M\n",
    "\n",
    "        SR = gp.GPSubmodel('SR', M, C, mesh = labundance)\n",
    "\n",
    "        frye_tau = Gamma('frye_tau', alpha=2., beta=1./(10.*(ry/4.)**2))\n",
    "\n",
    "        @deterministic\n",
    "        def frye_V(frye_tau=frye_tau):\n",
    "            \"\"\"\n",
    "            frye_V = 1/(frye_tau)\n",
    "            \"\"\"\n",
    "            return 1./(frye_tau)\n",
    "\n",
    "\n",
    "        @observed\n",
    "        def obs_frye(value=lfrye, mu = SR.f_eval, mesh=labundance, tau = frye_tau):\n",
    "            \"\"\"\n",
    "            The log of the frye count.\n",
    "            \"\"\"\n",
    "            return normal_like(value, mu, tau)\n",
    "\n",
    "        MCMC.__init__(self, locals())\n",
    "        \n",
    "        self.use_step_method(gp.GPEvaluationGibbs, SR, frye_V, obs_frye, verbose=0)\n",
    "\n",
    "\n",
    "    def plot_traces(self):\n",
    "        for object in [self.beta_0, self.beta_1, self.amp, self.scale, self.diff_degree, self.frye_tau]:\n",
    "            try:\n",
    "                y=object.trace()\n",
    "            except:\n",
    "                print(object.__name__)\n",
    "                break\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(y)\n",
    "            plt.title(object.__name__)\n",
    "\n",
    "\n",
    "    def plot_SR(self, n=3):\n",
    "        f_trace = self.SR.f.trace()\n",
    "        \n",
    "        gp.gpplots.plot_GP_envelopes(self.SR.f, self.plot_x, transx=np.log, transy=np.exp)\n",
    "\n",
    "        for i in range(n):\n",
    "            plt.plot(self.plot_x, np.exp(f_trace[i](np.log(self.plot_x))), label='draw %i'%i)\n",
    "\n",
    "        plt.plot(self.abundance, self.frye, 'k.', label='data', markersize=8)\n",
    "        plt.axis([self.abundance.min()*.1, self.abundance.max(), 0., self.frye.max()*2.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ss = SalmonSampler(abundance, frye)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ss.sample(10000, 5000, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ss.plot_SR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ss.plot_traces()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Processes in `scikit-learn`\n",
    "\n",
    "`scikit-learn` also provides a robust implementation of Gaussian processes, with the same familiar interface that its other machine learning methods use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specification of a `GaussianProcess` model class corresponds closely to that for PyMC. In place of `gp.Mean` and `gp.Covariance`, the scikit-learn implementation takes optional `regr` and `corr` arguments, respectively. These can either be callable functions or one of several built-in options.\n",
    "\n",
    "The `regr` argument specifies a regression function returning an array of outputs of the linear regression functional basis. The built-in functions are `constant` (default choice), `linear`, and `quadratic`.\n",
    "\n",
    "Similarly, the `corr` argument specifies a stationary autocorrelation function for any two points. The built-in correlation models include `absolute_exponential`, `squared_exponential` (default choice), `generalized_exponential`, `cubic`, and `linear`.\n",
    "\n",
    "Users can optionally specify autocorrelation paramter(s) `theta0`, which can either be scalar, or the size of the number of predictor variables. If arguments `thetaL` and `thetaU` are also provided, then these are taken as the lower and upper bounds on the autocorrelation parameters, with `theta0` the starting \"guess\" at a MLE of the parameter set.\n",
    "\n",
    "A `GaussianProcess` can be regularized by providing a `nugget` argument. Mathematically, the value represents the variance of the input values (*e.g.* observation error or other noise).\n",
    "\n",
    "Thus, the fitting of the regression and autocorrelation functions for the Gaussian process is highly automated in `scikits-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = abundance[:, None]\n",
    "y = frye\n",
    "\n",
    "G = GaussianProcess(theta0=1e-3, nugget=1e-12)\n",
    "G.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model over grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_pred = np.linspace(X.min(), X.max())[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred, MSE = G.predict(X_pred, eval_MSE=True)\n",
    "sigma = np.sqrt(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(X, y, 'r.', markersize=6, label=u'Observations')\n",
    "plt.plot(X_pred, y_pred, 'k:', label=u'Prediction')\n",
    "plt.fill(np.concatenate([X_pred, X_pred[::-1]]),\n",
    "        np.concatenate([y_pred - 1.9600 * sigma,\n",
    "                       (y_pred + 1.9600 * sigma)[::-1]]),\n",
    "        alpha=.3, fc='k', ec='None', label='95% confidence interval')\n",
    "plt.xlabel('Female abundance')\n",
    "plt.ylabel('Frye density')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model above was fit using arbitrarily-chosen hyperparameters. Typically, the hyperparameters are tuned to achieve the best model fit, according to selected criteria. As a preview of model building in `scikit-learn`, we can tune our GP by evaluating the model across a grid of plausible parameter values. The best combination will be chosen based on the minimum MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameter_grid = {'theta0': np.logspace(-7, 0), 'nugget': np.logspace(-5, 3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "gp = GaussianProcess()\n",
    "cv = GridSearchCV(gp, parameter_grid, scoring='mean_squared_error')\n",
    "cv.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gp_best = GaussianProcess(**cv.best_params_)\n",
    "gp_best.fit(X, y)\n",
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred, MSE = gp_best.predict(X_pred, eval_MSE=True)\n",
    "sigma = np.sqrt(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(X, y, 'r.', markersize=6, label=u'Observations')\n",
    "plt.plot(X_pred, y_pred, 'k:', label=u'Prediction')\n",
    "plt.fill(np.concatenate([X_pred, X_pred[::-1]]),\n",
    "        np.concatenate([y_pred - 1.9600 * sigma,\n",
    "                       (y_pred + 1.9600 * sigma)[::-1]]),\n",
    "        alpha=.3, fc='k', ec='None', label='95% confidence interval')\n",
    "plt.xlabel('Female abundance')\n",
    "plt.ylabel('Frye density')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Nashville daily temperatures\n",
    "\n",
    "The file `TNNASHVI.txt` in your data directory contains daily temperature readings for Nashville, courtesy of the [Average Daily Temperature Archive](http://academic.udayton.edu/kissock/http/Weather/). This data, as one would expect, oscillates annually. Use a Gaussian process to fit a regression model to this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "daily_temps = pd.read_table(\"../data/TNNASHVI.txt\", sep='\\s+', \n",
    "                            names=['month','day','year','temp'], na_values=-99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temps_2010 = daily_temps.temp[daily_temps.year>2010]\n",
    "temps_2010.plot(style='b.', figsize=(10,6), grid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.arange(len(temps_2010))[:, None]\n",
    "\n",
    "G_temp = GaussianProcess(corr='absolute_exponential', theta0=0.1, nugget=1)\n",
    "G_temp.fit(X, temps_2010.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(10,6))\n",
    "y_pred = G_temp.predict(X)\n",
    "plt.plot(X, y_pred, 'r-')\n",
    "plt.plot(temps_2010.values, 'b.', alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[Rasmussen, C. E., & Williams, C. K. I. (2005). Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning series). The MIT Press.](http://www.amazon.com/books/dp/026218253X)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}