{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#A local edited copy of the first section of Chris Fonnesbecks's [Bios8366 Section 5.1 gp notebook](https://github.com/fonnesbeck/Bios8366/blob/master/notebooks/Section5_1-Gaussian-Processes.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-parametric Bayes: Gaussian Processes\n",
    "\n",
    "Use of the term \"non-parametric\" in the context of Bayesian analysis is something of a misnomer. This is because the first and fundamental step in Bayesian modeling is to specify a *full probability model* for the problem at hand. It is rather difficult to explicitly state a full probability model without the use of probability functions, which are parametric. Bayesian non-parametric methods do not imply that there are no parameters, but rather that the number of parameters grows with the size of the dataset. In fact, Bayesian non-parametric models are *infinitely* parametric.\n",
    "\n",
    "## Building models with Gaussians\n",
    "\n",
    "It is easy to develop large, parametric models $p(y|\\theta)$ for large $\\theta$, particularly in a Bayesian context. However, this usually results in having to work with multidimensional integration over $\\theta$. \n",
    "\n",
    "One approach is to use MCMC; another is to represent your model with Gaussians. Normal distributions are easier to work with.\n",
    "\n",
    "$$p(x \\mid \\pi, \\Sigma) = (2\\pi)^{-k/2}|\\Sigma|^{-1/2} \\exp\\left\\{ -\\frac{1}{2} (x-\\mu)^{\\prime}\\Sigma^{-1}(x-\\mu) \\right\\}$$\n",
    "\n",
    "* marginals of multivariate normal distributions are normal\n",
    "\n",
    "$$p(x,y) = \\mathcal{N}\\left(\\left[{\n",
    "\\begin{array}{c}\n",
    "  {\\mu_x}  \\\\\n",
    "  {\\mu_y}  \\\\\n",
    "\\end{array}\n",
    "}\\right], \\left[{\n",
    "\\begin{array}{c}\n",
    "  {\\Sigma_x} & {\\Sigma_{xy}}  \\\\\n",
    "  {\\Sigma_{xy}^T} & {\\Sigma_y}  \\\\\n",
    "\\end{array}\n",
    "}\\right]\\right)$$\n",
    "\n",
    "$$p(x) = \\int p(x,y) dy = \\mathcal{N}(\\mu_x, \\Sigma_x)$$\n",
    "\n",
    "* conditionals of multivariate normals are normal\n",
    "\n",
    "$$p(x|y) = \\mathcal{N}(\\mu_x + \\Sigma_{xy}\\Sigma_y^{-1}(y-\\mu_y), \n",
    "\\Sigma_x-\\Sigma_{xy}\\Sigma_y^{-1}\\Sigma_{xy}^T)$$\n",
    "\n",
    "\n",
    "In some situations, we want to gain infernce about *functions*, rather than about, say, individuals or small vectors of parameters.\n",
    "\n",
    "A Gaussian process generalizes the multivariate normal to infinite dimension. It is considered a non-parametric approach, despite having an infinite number of parameters.\n",
    "\n",
    "**Gaussian Process**\n",
    "\n",
    "> An infinite collection of random variables, any finite subset of which have a Gaussian distribution.\n",
    "\n",
    "A Gaussian process is a ***disribution over functions***. Just as a multivariate normal distribution is completely specified by a mean vector and covariance matrix, a GP is fully specified by a mean *function* and a covariance *function*:\n",
    "\n",
    "$$p(x) \\sim \\mathcal{GP}(m(x), k(x,x^{\\prime}))$$\n",
    "\n",
    "It is the marginalization property that makes working with a Gaussian process feasible: we can marginalize over the infinitely-many variables that we are not interested in, or have not observed. \n",
    "\n",
    "For example, one specification of a GP might be as follows:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "m(x) &=0 \\\\\n",
    "k(x,x^{\\prime}) &= \\theta_1\\exp\\left(-\\frac{\\theta_2}{2}(x-x^{\\prime})^2\\right)\n",
    "\\end{aligned}$$\n",
    "\n",
    "here, the covariance function is a squared exponential, for which values of $x$ and $x^{\\prime}$ that are close together result in values of $k$ closer to 1 and those that are far apart return values closer to zero. (*spoiler*: we usually aren't very interested in the mean function!).\n",
    "\n",
    "For a finite number of points, the GP becomes a multivariate normal, with the mean and covariance as the mean functon and covariance function evaluated at those points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Basic exponential kernel\n",
    "exponential_kernel = lambda x, y, params: params[0] * \\\n",
    "    np.exp( -0.5 * params[1] * np.sum((x - y)**2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate a random sample from a GP with mean function zero and a double exponential covariance function as follows. \n",
    "\n",
    "In the context of Gaussian Processes, the covariance matrix is  referred to as the kernel (or Gram) matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Covariance matrix for MV normal\n",
    "covariance = lambda kernel, x, y, params: \\\n",
    "    np.array([[kernel(xi, yi, params) for xi in x] for yi in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.random.randn(10)*2\n",
    "theta = [1, 10]\n",
    "sigma = covariance(exponential_kernel, x, x, theta)\n",
    "y = np.random.multivariate_normal(np.zeros(len(x)), sigma)\n",
    "plt.plot(x, y, 'bo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate sample functions (*realizations*) sequentially, using the conditional:\n",
    "\n",
    "$$p(x|y) = \\mathcal{N}(\\mu_x + \\Sigma_{xy}\\Sigma_y^{-1}(y-\\mu_y), \n",
    "\\Sigma_x-\\Sigma_{xy}\\Sigma_y^{-1}\\Sigma_{xy}^T)$$\n",
    "\n",
    "This function implements the conditional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def conditional(x_new, x, y, fcov=exponential_kernel, params=theta):\n",
    "    B = covariance(fcov, x_new, x, params)\n",
    "    C = covariance(fcov, x, x, params)\n",
    "    A = covariance(fcov, x_new, x_new, params)\n",
    "    mu = np.linalg.inv(C).dot(B).T.dot(y)\n",
    "    sigma = A - np.linalg.inv(C).dot(B).T.dot(B)\n",
    "    return mu.squeeze(), sigma.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This band represents the prior mean function, plus and minus one standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sigma0 = exponential_kernel(0, 0, theta)\n",
    "xpts = np.arange(-3, 3, step=0.01)\n",
    "plt.errorbar(xpts, np.zeros(len(xpts)), yerr=sigma0, capsize=0)\n",
    "plt.ylim(-3, 3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by selecting a point at random, then drawing from an *unconditional* Gaussian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = [1.]\n",
    "y = [np.random.normal(scale=sigma0)]\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now calculate the conditional distribution, given the point that we just sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sigma1 = covariance(exponential_kernel, x, x, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(x, data, kernel, params, sigma, t):\n",
    "    k = [kernel(x, y, params) for y in data]\n",
    "    Sinv = np.linalg.inv(sigma)\n",
    "    y_pred = np.dot(k, Sinv).dot(t)\n",
    "    sigma_new = kernel(x, x, params) - np.dot(k, Sinv).dot(k)\n",
    "    return y_pred, sigma_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_pred = np.linspace(-3, 3, 1000)\n",
    "predictions = [predict(i, x, exponential_kernel, theta, sigma1, y) \n",
    "               for i in x_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot this to get an idea of what this looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred, sigmas = np.transpose(predictions)\n",
    "plt.errorbar(x_pred, y_pred, yerr=sigmas, capsize=0)\n",
    "plt.plot(x, y, \"ro\")\n",
    "plt.xlim(-3, 3); plt.ylim(-3, 3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can select a second point, conditional on the first point, using this new distribution. Let's arbitrarily select one at $x=-0.7$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu2, s2 = conditional([-0.7], x, y)\n",
    "y2 = np.random.normal(mu2, s2)\n",
    "y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.append(-0.7)\n",
    "y.append(y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the conditional distribution again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sigma2 = covariance(exponential_kernel, x, x, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = [predict(i, x, exponential_kernel, theta, sigma2, y) \n",
    "               for i in x_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred, sigmas = np.transpose(predictions)\n",
    "plt.errorbar(x_pred, y_pred, yerr=sigmas, capsize=0)\n",
    "plt.plot(x, y, \"ro\")\n",
    "plt.xlim(-3, 3); plt.ylim(-3, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the existing points constrain the selection of subsequent points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_more = [-2.1, -1.5, 0.3, 1.8, 2.5]\n",
    "mu, s = conditional(x_more, x, y)\n",
    "y_more = np.random.multivariate_normal(mu, s)\n",
    "y_more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x += x_more\n",
    "y += y_more.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sigma_new = covariance(exponential_kernel, x, x, theta)\n",
    "\n",
    "predictions = [predict(i, x, exponential_kernel, theta, sigma_new, y) \n",
    "               for i in x_pred]\n",
    "\n",
    "y_pred, sigmas = np.transpose(predictions)\n",
    "plt.errorbar(x_pred, y_pred, yerr=sigmas, capsize=0)\n",
    "plt.plot(x, y, \"ro\")\n",
    "plt.ylim(-3, 3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is exactly equivalent to adding points simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not restricted to a double-exponential covariance function. Here is a slightly more general function, which includes a constant and linear term, in addition to the exponential.\n",
    "\n",
    "$$k(x,x\\prime) = \\theta_1 \\exp\\left(-\\frac{\\theta_2}{2}(x-x^{\\prime})^2\\right) + \\theta_3 + \\theta_4 x^{\\prime} x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Exponential kernel, plus constant and linear terms\n",
    "exponential_linear_kernel = lambda x, y, params: \\\n",
    "    exponential_kernel(x, y, params[:2]) + params[2] + params[3] * np.dot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameters for the expanded exponential kernel\n",
    "theta = 2.0, 50.0, 0.0, 1.0\n",
    "\n",
    "# Some sample training points.\n",
    "xvals = np.random.rand(10) * 2 - 1\n",
    "\n",
    "# Construct the Gram matrix\n",
    "C = covariance(exponential_linear_kernel, xvals, xvals, theta)\n",
    "\n",
    "# Sample from the multivariate normal\n",
    "yvals = np.random.multivariate_normal(np.zeros(len(xvals)), C)\n",
    "\n",
    "plt.plot(xvals, yvals, \"ro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_pred = np.linspace(-1, 1, 1000)\n",
    "predictions = [predict(i, xvals, exponential_linear_kernel, theta, C, yvals) \n",
    "               for i in x_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y, sigma = np.transpose(predictions)\n",
    "plt.errorbar(x_pred, y, yerr=sigma, capsize=0)\n",
    "plt.plot(xvals, yvals, \"ro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[Rasmussen, C. E., & Williams, C. K. I. (2005). Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning series). The MIT Press.](http://www.amazon.com/books/dp/026218253X)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}